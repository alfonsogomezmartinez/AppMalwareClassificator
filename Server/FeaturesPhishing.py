#!/usr/bin/python
# -*- coding: utf-8 -*-
import re
from urlunshort3.urlunshort3 import UrlUnshortener
import tldextract
import favicon
from bs4 import BeautifulSoup 
import requests
from urllib.request import Request, urlopen
import socket
from urllib.parse import urlparse, urlencode
import whois
from datetime import datetime
import sys
import numpy as np

def URL_encoder(URL):
    URL_sol = np.ones((1,21), dtype=int)
    headers = {
        'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}    
    ext = tldextract.extract(URL)        
    try:
        r = requests.get(URL, headers = headers)
    except:
        URL_sol[0][0]=2
        return URL_sol
        
    html = r.content
    page_content = BeautifulSoup(html, "html.parser")
    whois_url = whois.whois(URL)
    
    # Using the IP Address  
    if re.match(r'http://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/.*', URL) == None and re.match(r'https://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}/.*', URL) == None and re.match(r'https://0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.*', URL) == None and re.match(r'http://0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.0[xX][0-9a-fA-F]+.*', URL) == None:
        URL_sol[0][0] = -1 
    
    # Long URL to Hide the Suspicious Part
    if len(URL) < 54:
        URL_sol[0][1] = -1
    elif len(URL) <= 75:
        URL_sol[0][1] = 0
        
    # Using URL Shortening Services “TinyURL”
    TinyURL = UrlUnshortener()
    if TinyURL.is_shortened(url=URL) == False:
        URL_sol[0][2] = -1
    
    # URL’s having “@” Symbol
    if URL.find('@') == -1:
        URL_sol[0][3] = -1
    
    # Redirecting using “//”
    if URL[7:].find('//') == -1:
        URL_sol[0][4] = -1
        
    # Adding Prefix or Suffix Separated by (-) to the Domain
    if ext.domain.find('-') == -1:
        URL_sol[0][5] = -1
        
    # Sub Domain and Multi Sub Domains
    if ext.subdomain.count('.') <= 1:
        URL_sol[0][6] = -1
    elif ext.subdomain.count('.') == 2:
        URL_sol[0][6] = 0
    
    ## HTTPS (Hyper Text Transfer Protocol with Secure Sockets Layer) //Consultar
    if URL[0:4].find('https') or URL[0:4].find('HTTPS'):
        #URL_sol[0][7] = -1
        URL_without = URL[8:]
    elif URL[0:4].find('http') or URL[0:4].find('HTTP'):
        URL_without = URL[7:]
        
    # Domain Registration Length
    if whois_url.expiration_date != None:
        if isinstance(whois_url.expiration_date, list):
            dif = whois_url.expiration_date[0]-datetime.now()
        else:
            dif = whois_url.expiration_date-datetime.now()        
        if dif.days >= 365:
            URL_sol[0][7] = -1
    
    # Favicon
    try:
        icon = favicon.get(URL)
        check_favicon = 0
        for x_url in icon:
            if tldextract.extract(x_url.url).domain == ext.domain:
                check_favicon = check_favicon + 1
        if check_favicon == len(icon):
            URL_sol[0][8] = -1 
    except:
        pass
    ##Using Non-Standard Port
    #a_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    
    #location = (URL_without, 21)
    #result_of_check = a_socket.connect_ex(location)
    #if result_of_check == 0:
        #URL_sol[0][9] = -1
    #else:
        #location = (URL_without, 22)
        #result_of_check = a_socket.connect_ex(location)
        #if result_of_check == 0:
            #URL_sol[0][9] = -1
        #else:
            #location = (URL_without, 23)
            #result_of_check = a_socket.connect_ex(location)
            #if result_of_check == 0:
                #URL_sol[0][9] = -1
            #else:
                #location = (URL_without, 80)
                #result_of_check = a_socket.connect_ex(location)
                #if result_of_check == 0:
                    #location = (URL_without, 443)
                    #result_of_check = a_socket.connect_ex(location)
                    #if result_of_check == 0:
                        #location = (URL_without, 445)
                        #result_of_check = a_socket.connect_ex(location)
                        #if result_of_check == 0:
                            #URL_sol[0][9] = -1
                        #else:
                            #location = (URL_without, 1433)
                            #result_of_check = a_socket.connect_ex(location)
                            #if result_of_check == 0:
                                #URL_sol[0][9] = -1
                            #else:    
                                #location = (URL_without, 1521)
                                #result_of_check = a_socket.connect_ex(location)
                                #if result_of_check == 0:
                                    #URL_sol[0][9] = -1
                                #else:   
                                    #location = (URL_without, 3306)
                                    #result_of_check = a_socket.connect_ex(location)
                                    #if result_of_check == 0:
                                        #URL_sol[0][9] = -1
                                    #else:  
                                        #location = (URL_without, 3389)
                                        #result_of_check = a_socket.connect_ex(location)
                                        #if result_of_check == 0:
                                            #URL_sol[0][9] = -1                                   
                    #else:
                        #URL_sol[0][9] = -1
                #else:
                    #URL_sol[0][9] = -1
    #a_socket.close()
    
    # The Existence of “HTTPS” Token in the Domain Part of the URL
    if URL_without.find('https') == -1 and URL_without.find('HTTP') == -1 and URL_without.find('HTTPS') == -1 and URL_without.find('http') == -1:
        URL_sol[0][9] = -1
    
    # Request URL
    request = 0
    if len(page_content.find_all("audio")) + len(page_content.find_all("video")) + len(page_content.find_all("img"))!=0:
        for img in page_content.find_all("img"):
                img_url = img.attrs.get("src")
                if not img_url:
                    # if img does not contain src attribute, just skip
                    request = request + 1
                else:
                    if tldextract.extract(img_url).domain.find(ext.domain) != -1 or  tldextract.extract(img_url).subdomain.find(ext.domain) != -1 or img_url == None:
                        request = request + 1    
                    else:
                        if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(img_url).domain+tldextract.extract(img_url).suffix).registrantorganization != None: 
                            if whois.whois("www"+tldextract.extract(img_url).domain+tldextract.extract(img_url).suffix).registrantorganization == whois_url.registrantorganization:
                                request = request + 1 
                        
        for video in page_content.find_all("video"):
                video_url = video.attrs.get("src")
                if not video_url:
                    # if video does not contain src attribute, just skip
                    request = request + 1
                else:
                    if tldextract.extract(video_url).domain.find(ext.domain) != -1 or  tldextract.extract(video_url).subdomain.find(ext.domain) != -1 or video_url == None:
                        request = request + 1
                    else:
                        if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(video_url).domain+tldextract.extract(video_url).suffix).registrantorganization != None:
                            if whois.whois("www"+tldextract.extract(video_url).domain+tldextract.extract(video_url).suffix).registrantorganization == whois_url.registrantorganization:
                                request = request + 1                 
                        
        for audio in page_content.find_all("audio"):
                audio_url = audio.attrs.get("src")
                if not audio_url:
                    # if audio does not contain src attribute, just skip
                    request = request + 1
                else:
                    if tldextract.extract(audio_url).domain.find(ext.domain) != -1 or  tldextract.extract(audio_url).subdomain.find(ext.domain) != -1 or audio_url == None:
                        request = request + 1
                    else:
                        if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(audio_url).domain+tldextract.extract(audio_url).suffix).registrantorganization != None:
                            if whois.whois("www"+tldextract.extract(audio_url).domain+tldextract.extract(audio_url).suffix).registrantorganization == whois_url.registrantorganization:
                                request = request + 1                 
        percent_rquest = 1 - request/(len(page_content.find_all("audio")) + len(page_content.find_all("video")) + len(page_content.find_all("img")))
        if percent_rquest < 0.22:
            URL_sol[0][10] = -1
        elif percent_rquest <= 61:
            URL_sol[0][10] = 0
    else:
        URL_sol[0][10] = 0
    
    # URL of Anchor
    if len(page_content.find_all("a")) != 0:
        request_a=0
        for a_tags in page_content.find_all("a"):
                a_tags_url = a_tags.attrs.get("href")
                if a_tags_url == None:
                    request_a = request_a + 1
                elif tldextract.extract(a_tags_url).domain.find(ext.domain) != -1 or  tldextract.extract(a_tags_url).subdomain.find(ext.domain) != -1:
                    request_a = request_a + 1
                else:
                    if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(a_tags_url).domain+tldextract.extract(a_tags_url).suffix).registrantorganization != None:
                        if whois.whois("www"+tldextract.extract(a_tags_url).domain+tldextract.extract(a_tags_url).suffix).registrantorganization == whois_url.registrantorganization:
                            request_a = request_a + 1                
        percent_rquest_a = 1 - request_a/(len(page_content.find_all("a")))
        if percent_rquest_a < 0.31:
            URL_sol[0][11] = -1
        elif percent_rquest_a <= 67:
            URL_sol[0][11] = 0
    else:
        URL_sol[0][11] = 0
    
    # Links in <Meta>, <Script> and <Link> tags//SRC meta
    link = page_content.findAll('link')
    script = page_content.findAll('script')
    if len(link)+len(script) != 0:
        cnt_script = 0;
        for element in script:
            if element.get('src') == None:
                cnt_script = cnt_script + 1
            elif tldextract.extract(element.get('src')).domain.find(ext.domain) != -1 or tldextract.extract(element.get('src')).subdomain.find(ext.domain) != -1:
                cnt_script = cnt_script + 1
            else:
                if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(element.get('src')).domain+tldextract.extract(element.get('src')).suffix).registrantorganization != None:                        
                    if whois.whois("www"+tldextract.extract(element.get('src')).domain+tldextract.extract(element.get('src')).suffix).registrantorganization == whois_url.registrantorganization:
                        cnt_script = cnt_script + 1            
                                
        cnt_link = 0;        
        for element in link:
            if element.get('href') == None:
                cnt_link = cnt_link + 1
            if tldextract.extract(element.get('href')).domain.find(ext.domain) != -1 or tldextract.extract(element.get('href')).subdomain.find(ext.domain) != -1:
                cnt_link = cnt_link + 1        
            else:
                if whois_url.registrantorganization != None and whois.whois("www"+tldextract.extract(element.get('href')).domain+tldextract.extract(element.get('href')).suffix).registrantorganization != None:
                    if whois.whois("www"+tldextract.extract(element.get('href')).domain+tldextract.extract(element.get('href')).suffix).registrantorganization == whois_url.registrantorganization:
                        cnt_link = cnt_link  + 1 
                    
        percent_of_link = 1-(cnt_link+cnt_script)/(len(link)+len(script))
        if percent_of_link < 0.17:
            URL_sol[0][12] = -1
        elif percent_of_link <= 0.81:
            URL_sol[0][12] = 0
    else:
        URL_sol[0][12] = 0
    
    # Server Form Handler (SFH)
    SFH = page_content.findAll('form')
    for element in SFH:
        if element.get('action') == None or element.get('action') == "about:blank":
            break
        elif element.get('src')==None:
            URL_sol[0][13] = -1    
        elif tldextract.extract(element.get('src')).domain.find(ext.domain) != -1 or tldextract.extract(element.get('src')).subdomain.find(ext.domain) != -1:
            URL_sol[0][13] = -1
        else:
            URL_sol[0][13] = 0
    
    ## Submitting Information to Email //Preguntar por la deteccion de la funcion mail() en php
    #SIE = page_content.findAll('a')
    #for element in SIE:
        #if element.get('href') != None:
            #if element.get('href').find('mailto:') == -1:
                #URL_sol[0][16] = -1
            
    # Abnormal URL
    if whois_url.name != None:
        if isinstance(whois_url.name, list):
            for i in whois_url:
                if i.upper().find(ext.domain.upper()) != -1:
                    URL_sol[0][14] = -1                
        elif whois_url.name.upper().find(ext.domain.upper()) != -1:
            URL_sol[0][14] = -1
            
    # Website Forwarding
    if len(r.history) - 1 <= 1:
        URL_sol[0][15] = -1
    elif len(r.history) - 1 < 4:
        URL_sol[0][15] = 0
    
    ## Status Bar Customization//Solo detecto el cambio pero no que provoca
    #if str(script).find("onMouseOver=") == None and str(script).find("window.status=") == None:
        #URL_sol[0][19] = -1
    
    ## Disabling Right Click//Solo detecto el cambio pero no que provoca
    #if str(script).find("event.button==2") == None and str(script).find("event.button == 2") == None:
        #URL_sol[0][20] = -1
    
    ## Using Pop-up Window//Dificil de detectar por el gran catalogo de evasiones
    #for element in link:
        #if element.get('href') != None:
            #if element.get('href').find("popup") != -1 and element.get('type').find('text') != -1:
                #URL_sol[0][21] = -1
                
    # IFrame Redirection
    iframe = page_content.findAll('iframe')
    if iframe == []:
        URL_sol[0][16] = -1
        
    # Age of Domain
    if whois_url.creation_date != None:
        if isinstance(whois_url.creation_date, list):
            delta = datetime.now()-whois_url.creation_date[0]
        else:
            delta = datetime.now()-whois_url.creation_date
        if delta.days > 182:
            URL_sol[0][17] = -1
    
    # DNS Record
    if whois_url.registrar != None:
        URL_sol[0][18] = -1
    
    # Website Traffic
    alexa_base_url = 'https://alexa.com/siteinfo/'
    site_name = URL
    site_name.lower()
    
    url_for_rank = alexa_base_url + site_name
    
    # Request formatted url for rank(s)
    page = requests.get(url_for_rank)
    soup = BeautifulSoup(page.content, 'html.parser')
    
    # select the data with class='rank-global' and the class='data'
    global_rank = soup.select('.rank-global .data')
    # Display Global rank safely
    try:
        match = re.search(r'[\d,]+', global_rank[0].text.strip())
        if len(match.group()) <= 6:
            URL_sol[0][19] = -1
    except:
        URL_sol[0][19] = 1
    
    # PageRank//Pagerank
    
    
    # Google Index
    query = {'q': 'info:' + URL}
    google = "https://www.google.com/search?" + urlencode(query)
    req = Request(google, headers={'User-Agent': 'Mozilla/5.0'})
    webpage = urlopen(req).read().decode('ISO-8859-1')
    soup = BeautifulSoup(webpage, "html.parser")
    main = soup.findAll(id="main")
    for element in main:
        div = element.findAll("a")
        for element in div:
            if element.get("href").find("/url?q=" + URL) != -1:
                URL_sol[0][20] = -1
                break
    
    # Number of Links Pointing to Page //dIFICil de obtener
    
    # Statistical-Reports Based Feature // Incapaz de consultar
    
    return URL_sol